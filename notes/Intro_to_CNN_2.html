<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Intro to CNN 2 - My Kindle Notes</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="../css/mobile.css">
    <link rel="stylesheet" href="../css/note-page.css">
    <link rel="stylesheet" href="../css/responsive.css">
    <link rel="icon" href="../assets/favicon.ico" type="image/x-icon">
    <meta name="description" content="Kindle highlights from Intro to CNN 2">
</head>
<body class="note-page">
    <header>
        <div class="container">
            <div class="header-content">
                <h1 class="site-title">Intro to CNN 2</h1>
                <a href="../index.html" class="back-button">Back to Home</a>
            </div>
        </div>
    </header>

    <main>
        <div class="container">
            <div class="note-content">
                <p>After each conv layer, it is convention to apply a nonlinear layer (or activation layer) immediately afterward.The purpose of this layer is to introduce nonlinearity to a system that basically has just been computing linear operations during the conv layers (just element wise multiplications and summations).In</p>
<p>pooling layer. It is also referred to as a downsampling layer.</p>
<p>The intuitive reasoning behind this layer is that once we know that a specific feature is in the original input volume (there will be a high activation value), its exact location is not as important as its relative location to the other features.</p>
<p>This layer “drops out” a random set of activations in that layer by setting them to zero. Simple as that. Now, what are the benefits of such a simple and seemingly unnecessary and counterintuitive process? Well, in a way, it forces the network to be redundant. By that I mean the network should be able to provide the right classification or output for a specific example even if some of the activations are dropped out. It makes sure that the network isn’t getting too “fitted” to the training data and thus helps alleviate the overfitting problem.</p>
<p>A network in network layer refers to a conv layer where a 1 x 1 size filter is used.</p>
<p>Transfer learning is the process of taking a pre-trained model (the weights and parameters of a network that has been trained on a large dataset by somebody else) and “fine-tuning” the model with your own dataset. The idea is that this pre-trained model will act as a feature extractor. You will remove the last layer of the network and replace it with your own classifier (depending on what your problem space is). You then freeze the weights of all the other layers and train the network normally (Freezing the layers means not changing the weights during gradient descent/optimization).</p>
<p>Approaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques.</p>
<p>Notes: 1) for example inserting a row or rotating the image etc</p>
<p>Some popular augmentations people use are grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations,</p>

            </div>
        </div>
    </main>

    <footer>
        <div class="container">
            <div class="footer-content">
                <p class="footer-text">My Kindle Notes - A collection of book highlights</p>
                <a href="../index.html" class="back-link">Back to Home</a>
            </div>
        </div>
    </footer>

    <div class="back-to-top" id="back-to-top">↑</div>

    <script src="../js/note-page.js"></script>
</body>
</html>
